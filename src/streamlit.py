import streamlit as st
import whisper
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.set_page_config(page_title="ChatPPT Assistant", layout="wide")
st.title("ğŸ¤ ChatPPT Assistant: Voice & Text Interaction")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼ˆä¿å­˜èŠå¤©å†å²ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = []

# ä¾§è¾¹æ ï¼šæ¨¡å‹é€‰æ‹©å’Œé…ç½®
with st.sidebar:
    st.header("Settings")
    # é€‰æ‹©è¾“å…¥æ¨¡å¼ï¼šè¯­éŸ³æˆ–æ–‡æœ¬
    input_mode = st.radio("Input Mode", ["Text", "Voice"])

# --- æ¨¡å‹åŠ è½½ï¼ˆç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰ ---
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")  # å¯é€‰ "small", "medium"

@st.cache_resource
def load_minicpm_model():
    model_name = "openbmb/MiniCPM-2B-dpo-fp16"  # æ›¿æ¢ä¸ºå®é™…æ¨¡å‹è·¯å¾„
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
    return model, tokenizer

whisper_model = load_whisper_model()
minicpm_model, minicpm_tokenizer = load_minicpm_model()

# --- åŠŸèƒ½å‡½æ•° ---
def transcribe_audio(audio_path):
    """Whisper è¯­éŸ³è½¬æ–‡æœ¬"""
    result = whisper_model.transcribe(audio_path)
    return result["text"]

def generate_text(prompt):
    """MiniCPM ç”Ÿæˆæ–‡æœ¬"""
    inputs = minicpm_tokenizer(prompt, return_tensors="pt")
    outputs = minicpm_model.generate(**inputs, max_length=200)
    return minicpm_tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- ä¸»äº¤äº’é€»è¾‘ ---
# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è¾“å…¥åŒºåŸŸ
if input_mode == "Voice":
    uploaded_audio = st.file_uploader("Upload Audio", type=["wav", "mp3"])
    if uploaded_audio:
        with st.spinner("Transcribing audio..."):
            text = transcribe_audio(uploaded_audio.name)
            st.session_state.messages.append({"role": "user", "content": text})
            with st.chat_message("user"):
                st.markdown(text)
else:
    if prompt := st.chat_input("Type your question here..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

# ç”Ÿæˆå›å¤
if st.session_state.messages and st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            full_response = generate_text(st.session_state.messages[-1]["content"])
            st.markdown(full_response)
    st.session_state.messages.append({"role": "assistant", "content": full_response})
